---
title: "ResultsNPS"
author: "Franco Peschiera"
date: "August 8, 2019"
output:
  html_document: default
  pdf_document: default
params:
  size: get_3_tasks_aggresive_perc_add
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
# setwd('../')
# browser()
source('nps_reports/functions.R')
source('nps_reports/datasets.R')
quant_max <- 0.95
size <- params$size
df <- do.call(size, args=list())
value_filt_tails <- function(value, each_tail) value %>% between(., quantile(., c(each_tail[1])), quantile(., c(1-each_tail[2])))

```

## Nomenclature

* `cuts` model with added learning constraints.
* `base` model without any added constraints.
* `dif_abs` absolute distance between the two models for the shown quantity.
* `dif_rel` relative distance between the two models for the shown quantity. Always using (cuts-base)/base.
* `time_mean` mean time.
* `time_medi` median time.
* `errors_q95` number of errors in the percentile 95%.

## General statistics

The following table shows the general statistics about the instances solved.

```{r stats_general}

summary_stats <- get_summary(df)
summary_stats %>% kable

```

## Solution quality

### Optimality degradation

```{r quality_degr}

quality_degr_all <- get_quality_degr_2(df)

t1_rel <- quality_degr_all %>% filter(experiment=='cuts') %>% use_series(dif_perc)
t1_abs <- quality_degr_all %>% filter(experiment=='cuts') %>% use_series(dif)
quality_degr_quant <- t1_rel %>% quantile()
quality_degr_quant95 <- t1_rel %>% quantile(quant_max)
quality_degr_quant95_abs <- t1_abs %>% quantile(quant_max)

right_tail <- 0.10
quality_degr_all_filt <- 
  quality_degr_all %>% 
  group_by(experiment) %>% 
  filter(dif_perc %>% value_filt_tails(c(0, right_tail)))

```
We compare the times where all models return an **optimal solution** (`r quality_degr_all %>% distinct(instance) %>% nrow` instances).

<!-- For `r quant_max*100`\% of the `r quality_degr_nrow` (`r (quant_max*quality_degr_nrow) %>% floor`) instances that comply with this condition, the loss in the objective function was `r quality_degr_quant95 %>% round(2)`\% or less from the optimal. This corresponds to an absolute value of `r quality_degr_quant95_abs %>% round(2)`. As a reference, the absolute gap tolerance used in the solving was of 10. -->


The following boxplot shows the distribution of the gap from the models with respect to the `base` model.

```{r}
ggplot(data=quality_degr_all_filt, aes(x=experiment, y=dif_perc)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab('% difference in objective') + theme(text = element_text(size=20)) + coord_flip()

```

### Mean quality comparison

```{r quality_perf}

quality_perf_all <- get_quality_perf_2(df)

quality_perf <- quality_perf_all %>% filter(experiment=='cuts')
mean_dif <- quality_perf$dif_perc %>%  mean
per_cut <- sum(quality_perf$dif_perc<0)/length(quality_perf$dif_perc)
q95_cut <- quality_perf$dif_perc %>% quantile(0.95)

```

If we compare among all cases where an **integer solution** (`r quality_perf_all %>% distinct(instance) %>% nrow` instances) was found in the two models, we can obtain an estimate of quality performance for the 1 hour solving time.

```{r}

# this takes some % from each side
each_tail <- 0.15

quality_perf_all_filt <- 
  quality_perf_all %>% 
  group_by(experiment) %>% 
  filter(dif_perc %>% value_filt_tails(c(each_tail, each_tail)))
```

The following graph shows the distribution of relative gaps between the best integer solutions found in each of the two models. A negative relative gap means the model outperformed the `base` model in that particular instance. Both extreme tails (`r (each_tail*100) %>% round(2)`\%) have been taken out to help better visualize it.

```{r}

ggplot(data=quality_perf_all_filt, aes(x=experiment, y=dif_perc)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab('% difference in objective') + theme(text = element_text(size=20)) + coord_flip()


```

<!-- Of the `r quality_perf_nrow` instances were an **integer solution** was found in all model. -->
<!-- the average relative gap between the `cuts` model and the `base` model was of `r mean_dif %>% round(2)`\%. This means that, in average solutions of the `cuts` model were `r (-mean_dif) %>% round(2)`\% better than the `base` model. -->

<!-- In addition, in `r (per_cut*100) %>% round(2)`\% of instances (`r (quality_perf_nrow*per_cut) %>% floor` of `r quality_perf_nrow`) the `cuts` model outperform the solution from the base model. 95\% of instances are less than `r q95_cut %>% round(2)`\% away from the best known solution. -->

## Performance analysis

### Average solving time

Just like in the mean quality comparison, we compare among cases where all models found a **integer solution**.

```{r time_perf}

comparison_table <- get_time_perf_integer(df)

comparison_table_reorder <- get_time_perf_integer_reorder(df)

mean_times <- 
    comparison_table %>% 
    group_by(scenario, experiment) %>% 
    summarise(time_mean = mean(time), 
              time_medi = median(time)) %>% 
    aux_compare

```

The difference in average and median times can be seen in the following table:

```{r}
mean_times %>% kable
```

In order to get more intuition in the performance per instance, we present a couple of graphs.

The following graph shows the solving times in seconds for each of the `r comparison_table %>% distinct(instance) %>% nrow` instances were an integer solution was found in all cases. If we reorder all models from fastest to slowest, we gain more insight on the distribution of solving times.


<!-- ```{r time_perf_graph}  -->
<!-- ggplot(data=comparison_table, aes(x=instance, y=time, color=experiment)) + theme_minimal() + geom_point(size=0.5) + theme(text = element_text(size=20)) + guides(color = guide_legend(override.aes = list(size=5))) -->
<!-- ``` -->


```{r time_perf_graph2} 
ggplot(data=comparison_table_reorder, aes(x=percentage, y=time, color=experiment)) + theme_minimal() + geom_point(size=0.5) + ggplot2::xlab('instance percentage') + theme(text = element_text(size=20)) + guides(color = guide_legend(override.aes = list(size=5)))
```

### Average solving time to optimality

Here, we measure the time it took for each model to reach an **optimal solution.** We compare among instances where all models obtained and optimal solution.

```{r time_perf_optim}

comparison_table <- get_time_perf_optim(df)

```

```{r time_perf_optim_graph}
comparison_table %>% kable
```

## Feasiblility analysis

There are two indicators to measure if a model is too constrained. The first one is the fact the model is infeasible. The second one is the accomplishement of the soft constraints in the solution.

```{r infeasible_list}
infeasible_instances <- get_infeasible_instances(df)
infeasible_stats <- get_infeasible_stats(df)
infeasible_times <- get_infeasible_times(df)
```

### Infeasible solutions

We take all `r infeasible_instances %>% distinct(instance) %>% nrow` **infeasible solutions** from the models and obtain statistics on their solution on the base model.

The below table shows the status obtained in the base model. The number of infeasible solutions is very close in both models and for all instances that were identified by infeasible in the a model, the `base` model was unable to find an integer solution.

```{r infeasible_stats}
infeasible_stats %>% kable
```

In addition, the times to detect the infeaseability of a given instance have been greatly reduced (to half, approximately) as can be see in the following table:

```{r infeasible_times}
infeasible_times %>% kable
```


### Soft constraints performance

Regarding the accomplishment of soft constraints, we can analyze for mutually **optimal solutions**, the quantity of violations in their solutions.

```{r soft_constraints}
errors_stats <- get_soft_constraints(df, quant_max)

```

The following table shows the difference in soft constraints errors.

```{r soft_constraints_table}
errors_stats %>% kable
```

```{r variances}
variances_all <- get_variances(df)
# var_per <- variances_all %>% filter(experiment=='cuts') %>% use_series(dif_perc)
# mean_var_reduction <- var_per %>% mean
# quant95_var_reduction <- var_per %>% quantile(quant_max)

right_tail <- 0.1

variances_all_filt <- 
  variances_all %>% 
  group_by(experiment) %>% 
  filter(dif_perc %>% value_filt_tails(c(0, right_tail)))

```

### Making aircraft usage more homogeneous

The variance in the frequency of maintenances is a side objective that was not been taken into account previously. The higher the variance, the more difference there is between amount of flight hours between one aircraft and another.
To calculate it, we have, for each fleet type, measured the variance of the distance between maintenances. And we have summed all variances for all fleet types. We do this analysis for all integer solutions obtained.
As can be seen, in almost all of the cases there is a reduction in the variance.

```{r graph_variances}

# qplot(var_per, xlab='')

ggplot(data=variances_all_filt, aes(x=experiment, y=dif_perc)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab('Difference in variance (in % of base case)') + theme(text = element_text(size=20)) + coord_flip()

```
