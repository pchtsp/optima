---
title: "ResultsNPS"
author: "Franco Peschiera"
date: "August 8, 2019"
output:
  html_document: default
  pdf_document: default
params:
  size: get_3_tasks_aggresive_perc_add
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
source('nps_reports/graphs_NPS.R')
source('nps_reports/datasets_NPS.R')
quant_max <- 0.95
size <- params$size

# alternative 1:
# exp_list <- c('IT000125_20190808', 'IT000125_20190812')
# df_original <- compare_sto$get_df_comparison(exp_list)
# df_original %<>% filter(scenario=='numparalleltasks_3' | experiment==1)
# alternative 2:
# compare_sto <- get_compare_sto()
# df_original <- compare_sto$get_instances[[size]]()
# df <- df_original %>% mutate(experiment=if_else(experiment==0, 'cuts', 'base'))
# alternative 3:
df <- do.call(size, args=list())
# df <- get_4_tasks_aggressive()

```

## Nomenclature

* `cuts` model with added learning constraints.
* `base` model without any added constraints.
* `dif_abs` absolute distance between the two models for the shown quantity.
* `dif_rel` relative distance between the two models for the shown quantity. Always using (cuts-base)/base.
* `time_mean` mean time.
* `time_medi` median time.
* `errors_q95` number of errors in the percentile 95%.

## General statistics

The following table shows the general statistics about the instances solved.

```{r stats_general}

summary_stats <- get_summary(df)
summary_stats %>% kable

```

## Solution quality

### Optimality degradation

We compare the times where both models return an **optimal solution**.

```{r quality_degr}

t1 <- get_quality_degr(df)
t1_rel <- t1 %>% filter(experiment=='cuts') %>% use_series(dist_min_perc)
t1_abs <- t1 %>% filter(experiment=='cuts') %>% use_series(dist_min)

quality_degr_quant <- t1_rel %>% quantile()
quality_degr_quant95 <- t1_rel %>% quantile(quant_max)
quality_degr_nrow <- t1_rel %>% length()
quality_degr_quant95_abs <- t1_abs %>% quantile(quant_max)

```

For `r quant_max*100`\% of the `r quality_degr_nrow` (`r (quant_max*quality_degr_nrow) %>% floor`) instances that comply with this condition, the loss in the objective function was `r quality_degr_quant95 %>% round(2)`\% or less from the optimal. This corresponds to an absolute value of `r quality_degr_quant95_abs %>% round(2)`. As a reference, the absolute gap tolerance used in the solving was of 10.

```{r}
t1_rel_filt <- t1_rel[t1_rel<20]
filtered_quant <- (1 - (t1_rel_filt %>% length) / (t1_rel %>% length))*100
```


The following graph shows the distribution of the gap from the `cuts` model with the `base` model. `r filtered_quant %>% round(2)`\% of outliers were taken out from the right tail.

```{r}
qplot(t1_rel_filt, xlab='Relative gap (in %) among optimal solutions.')
```

### Mean quality comparison

If we compare among all cases where an **integer solution** was found in the two models, we can obtain an estimate of quality performance for the 1 hour solving time.

```{r quality_perf}

quality_perf <- get_quality_perf(df)
mean_dif <- quality_perf$dif_perc %>%  mean
per_cut <- sum(quality_perf$dif_perc<0)/length(quality_perf$dif_perc)
q95_cut <- quality_perf$dif_perc %>% quantile(0.95)

quality_perf_nrow <- quality_perf %>% distinct(instance) %>% nrow

```

```{r}
filtered_q_perf <- quality_perf %>% filter(between(dif_perc, -10, 10))
filtered_quant <- (1 - (filtered_q_perf %>% nrow)/ (quality_perf %>% nrow))*100
```
The following graph shows the distribution of relative gaps between the best integer solutions found in each of the two models. A negative relative gap means the `cuts` model outperformed the `base` model in that particular instance. Both extreme tails (`r filtered_quant %>% round(2)`\%) have been taken out to help better visualize it.

```{r}
qplot(filtered_q_perf$dif_perc, xlab='Relative gap (in %) among integer solutions.')
```


Of the `r quality_perf_nrow` instances were an **integer solution** was found in both models, the average relative gap between the `cuts` model and the `base` model was of `r mean_dif %>% round(2)`\%. This means that, in average solutions of the `cuts` model were `r (-mean_dif) %>% round(2)`\% better than the `base` model.

In addition, in `r (per_cut*100) %>% round(2)`\% of instances (`r (quality_perf_nrow*per_cut) %>% floor` of `r quality_perf_nrow`) the `cuts` model outperform the solution from the base model. 95\% of instances are less than `r q95_cut %>% round(2)`\% away from the best known solution.

## Performance analysis

### Average solving time

Just like in the mean quality comparison, we compare among cases where both models found a **integer solution**.

```{r time_perf}

comparison_table <- get_time_perf_integer(df)

comparison_table_reorder <- get_time_perf_integer_reorder(df)

mean_times <- 
    comparison_table %>% 
    group_by(scenario, experiment) %>% 
    summarise(time_mean = mean(time), 
              time_medi = median(time)) %>% 
    aux_compare

```

The difference in average and median times can be seen in the following table:

```{r}
mean_times %>% kable
```

In order to get more intuition in the performance per instance, we present a couple of graphs.

The following graph shows the solving times in seconds for each of the `r comparison_table %>% distinct(instance) %>% nrow` instances were an integer solution was found in both cases. In red are the times for the base model. In green the times for the model with the added cuts.

```{r time_perf_graph} 
ggplot(data=comparison_table, aes(x=instance, y=time, color=experiment)) + theme_minimal() + geom_point(size=0.5)
```

If we reorder both models from fastest to slowest, we lose the comparison of individual instances but we gain more insight on the distribution of solving times.

```{r time_perf_graph2} 
ggplot(data=comparison_table_reorder, aes(x=percentage, y=time, color=experiment)) + theme_minimal() + geom_point(size=0.5) + ggplot2::xlab('instance percentage')
```

### Average solving time to optimality

Here, we measure the time it took for each model to reach an **optimal solution.** We compare among instances where both models obtained and optimal solution.

```{r time_perf_optim}

comparison_table <- get_time_perf_optim(df)

```

```{r time_perf_optim_graph}
comparison_table %>% kable
```

## Feasiblility analysis

There are two indicators to measure if a model is too constrained. The first one is the fact the model is infeasible. The second one is the accomplishement of the soft constraints in the solution.

```{r infeasible_list}
infeasible_instances <- get_infeasible_instances(df)
infeasible_stats <- get_infeasible_stats(df)
```

### Infeasible solutions

We take all `r infeasible_instances %>% nrow` **infeasible solutions** from the cuts model and obtain statistics on their solution on the base model.

The below table shows the status obtained in the base model. The number of infeasible solutions is very close in both models and for all instances that were identified by infeasible in the `cuts` model, the `base` model was unable to find an integer solution.

```{r infeasible}
infeasible_stats %>% kable
```

### Soft constraints performance

Regarding the accomplishment of soft constraints, we can analyze for mutually **integer solutions**, the quantity of violations in their solutions.

```{r soft_constraints}
errors_stats <- get_soft_constraints(df, quant_max)

```

The following table shows the difference in soft constraints errors.

```{r soft_constraints_table}
errors_stats %>% kable
```
