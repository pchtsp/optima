---
title: "ResultsNPS"
author: "Franco Peschiera"
date: "August 8, 2019"
output:
  html_document: default
  pdf_document: default
params:
  size: get_3_tasks_aggresive_perc_add
  scale: log2
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
# setwd('../')
# browser()
source('nps_reports/functions.R')
source('nps_reports/datasets.R')
default_tail <- 0.05
really_small_number <- 0.000001
size <- params$size
df <- do.call(size, args=list())
value_filt_tails <- function(value, each_tail) value %>% between(., quantile(., c(each_tail[1])), quantile(., c(1-each_tail[2])))

scaleFUN <- function(x) sprintf("%.2f", x)
scale <- params$scale



```

## Nomenclature

* `cuts` model with added learning constraints.
* `base` model without any added constraints.
* `time_mean` mean time.
* `time_medi` median time.
* `errors_q95` number of errors in the percentile 95%.

## General statistics

The following table shows the general statistics about the instances solved.

```{r stats_general}

summary_stats <- get_summary(df)
summary_stats %>% kable

```

## Solution quality

### Optimality degradation

```{r quality_degr}

quality_degr_all <- get_quality_degr_2(df)

right_tail <- default_tail
quality_degr_all_filt <- 
  quality_degr_all %>% 
  mutate(dif_perc = if_else(dif_perc<= 0, really_small_number, dif_perc)) %>%
  filter(dif_perc %>% value_filt_tails(c(0, right_tail)))

```
We compare the times where all models return an **optimal solution** (`r quality_degr_all %>% distinct(instance) %>% nrow` instances).


The following boxplot shows the distribution of the gap from the models with respect to the `base` model. We have replace all negative values for a very small one close to zero. We have taken (`r right_tail*100`\%) values from the right side to better visualize the output.

```{r}
if (scale!='identity'){
  ylab <- sprintf('%% difference in objective (%s scale)', scale)  
} else {
  ylab <- '% difference in objective'
}

  
ggplot(data=quality_degr_all_filt, aes(x=experiment, y=dif_perc)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab(ylab) + theme(text = element_text(size=20)) + coord_flip() + scale_y_continuous(trans=scale, labels=scaleFUN)

```

### Mean quality comparison

```{r quality_perf}

quality_perf_all <- get_quality_perf_2(df)

```

If we compare among all cases where an **integer solution** (`r quality_perf_all %>% distinct(instance) %>% nrow` instances) was found in the two models, we can obtain an estimate of quality performance for the 1 hour solving time.

```{r}

# this takes some % from each side
each_tail <- default_tail

quality_perf_all_filt <- 
  quality_perf_all %>% 
  mutate(dif_perc = if_else(dif_perc<= 0, really_small_number, dif_perc)) %>%
  filter(dif_perc %>% value_filt_tails(c(each_tail, each_tail)))
```

The following graph shows the distribution of relative gaps between the best integer solutions found in each of the two models. A negative relative gap means the model outperformed the `base` model in that particular instance. We have taken (`r each_tail*100`\%) values from each side to better visualize the output.

```{r quality_perf_plot}

if (scale!='identity'){
  ylab <- sprintf('%% difference in objective (%s scale)', scale)  
} else {
  ylab <- '% difference in objective'
}

ggplot(data=quality_perf_all_filt, aes(x=experiment, y=dif_perc)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab(ylab) + theme(text = element_text(size=20)) + coord_flip() + scale_y_continuous(trans=scale, labels=scaleFUN)


```


## Performance analysis

### Average solving time

Just like in the mean quality comparison, we compare among **all** cases.

```{r time_perf}

comparison_table <- get_time_perf_integer(df)

comparison_table_reorder <- get_time_perf_integer_reorder(df)

mean_times <- 
    comparison_table %>% 
    group_by(scenario, experiment) %>% 
    summarise(time_mean = mean(time), 
              time_medi = median(time)) %>% 
    aux_compare

```

The difference in average and median times can be seen in the following table:

```{r}
mean_times %>% kable
```

In order to get more intuition in the performance per instance, we present a couple of graphs.

The following graph shows the solving times in seconds for each of the `r comparison_table %>% distinct(instance) %>% nrow` instances. If we reorder all models from fastest to slowest, we gain more insight on the distribution of solving times.


```{r time_perf_graph2, out.width='100%'}
ggplot(data=comparison_table_reorder, aes(x=percentage, y=time, color=experiment)) + theme_minimal() + geom_point(size=0.5) + ggplot2::xlab('instance percentage') + theme(text = element_text(size=20)) + guides(color = guide_legend(override.aes = list(size=5)))
```

### Average solving time to optimality

Here, we measure the time it took for each model to reach an **optimal solution.** We compare among instances where all models obtained and optimal solution.

```{r time_perf_optim}

comparison_table <- get_time_perf_optim(df)

```

```{r time_perf_optim_graph}
comparison_table %>% kable
```

## Feasiblility analysis

There are two indicators to measure if a model is too constrained. The first one is the fact the model is infeasible. The second one is the accomplishement of the soft constraints in the solution.

```{r infeasible_list}
infeasible_instances <- get_infeasible_instances(df)
infeasible_stats <- get_infeasible_stats(df)
infeasible_times <- get_infeasible_times(df)
```

### Infeasible solutions

We take all `r infeasible_instances %>% distinct(instance) %>% nrow` **infeasible solutions** from the models and obtain statistics on their solution on the base model.

The below table shows the status obtained in the base model. The number of infeasible solutions is very close in both models and for all instances that were identified by infeasible in the a model, the `base` model was unable to find an integer solution.

```{r infeasible_stats}
infeasible_stats %>% kable
```

In addition, the times to detect the infeaseability of a given instance can be seen in the following table:

```{r infeasible_times}
infeasible_times %>% kable
```


### Soft constraints performance

```{r soft_constraints}
right_tail <- default_tail
errors_stats <- get_soft_constraints(df)
errors_all <- get_soft_constraints_2(df) %>% 
  mutate(dif= if_else(dif<=0, really_small_number, dif))

errors_all_filt <- 
  errors_all %>% 
  filter(dif %>% value_filt_tails(c(0, right_tail)))

```


Regarding the accomplishment of soft constraints, we can analyze for mutually **optimal solutions**, the quantity of violations in their solutions. For each model and instance, we subtract the `base` model errors. We show only the extra errors (in number) compared to the `base` case. We have taken (`r right_tail*100`\%) values from he right side to better visualize the output.

The following table shows the difference in soft constraints errors.

```{r soft_constraints_table}
errors_stats %>% kable
```

And the complete distributions:

```{r soft_constraints_plot}

if (scale!='identity'){
  ylab <- sprintf('absolute difference in errors (%s scale)', scale)
} else {
  ylab <- sprintf('absolute difference in errors')
}

ggplot(data=errors_all_filt, aes(x=experiment, y=dif)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab(ylab) + theme(text = element_text(size=20)) + coord_flip() + scale_y_continuous(trans=scale, labels=scaleFUN)



```

```{r variances}
variances_all <- get_variances(df)

right_tail <- 0.1

variances_all_filt <- 
  variances_all %>% 
  group_by(experiment) %>%
  filter(dif_perc %>% value_filt_tails(c(0, right_tail)))

```

### Making aircraft usage more homogeneous

The variance in the frequency of maintenances is a side objective that was not been taken into account previously. The higher the variance, the more difference there is between amount of flight hours between one aircraft and another.
To calculate it, we have, for each fleet type, measured the variance of the distance between maintenances. And we have summed all variances for all fleet types. We do this analysis for all integer solutions obtained.
To better visualize the results, we have taken `r right_tail*100`\% from the right tail.

```{r graph_variances}

# qplot(var_per, xlab='')

ggplot(data=variances_all_filt, aes(x=experiment, y=dif_perc)) + theme_minimal() + geom_boxplot() + xlab('Experiment') + ylab('Difference in variance (in % of base case)') + theme(text = element_text(size=20)) + coord_flip()

```
